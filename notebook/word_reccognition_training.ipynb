{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7NtJ4lPBS8MQ"
      },
      "outputs": [],
      "source": [
        "!wget -q https://git.io/J0fjL -O IAM_Words.zip\n",
        "!unzip -qq IAM_Words.zip\n",
        "!\n",
        "!mkdir data\n",
        "!mkdir data/words\n",
        "!tar -xf IAM_Words/words.tgz -C data/words\n",
        "!mv IAM_Words/words.txt data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JQbdh0GVVq3n"
      },
      "outputs": [],
      "source": [
        "!unzip --qq sagemaker.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uFQrvNkhDdA8",
        "outputId": "82f965e2-75d7-48eb-af58-8d432413bf20"
      },
      "outputs": [],
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow==2.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TeNK71cSy8y"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import StringLookup\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzKr9KQxSy8z",
        "outputId": "b22f4419-43d5-49bf-8d08-b6254de98088"
      },
      "outputs": [],
      "source": [
        "base_path = \"data\"\n",
        "words_list = []\n",
        "\n",
        "words = open(f\"{base_path}/words.txt\",\"r\").readlines()\n",
        "for line in words:\n",
        "  if line[0] == \"#\":\n",
        "    continue\n",
        "  if line.split(\" \")[1] != \"err\":\n",
        "    words_list.append(line)\n",
        "\n",
        "print(len(words_list))\n",
        "words_list = [entry for entry in words_list if 'r06-022-03-05 ok 184 924 1304 132 29 QL' not in entry]\n",
        "print(len(words_list))\n",
        "np.random.shuffle(words_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iWqucSASy80",
        "outputId": "a8089f37-3a2f-4f52-93d4-b458829a83bb"
      },
      "outputs": [],
      "source": [
        "split_idx = int(0.9 * len(words_list))\n",
        "train_samples = words_list[:split_idx]\n",
        "test_samples = words_list[split_idx:]\n",
        "\n",
        "val_split_idx = int(0.5 * len(test_samples))\n",
        "validation_samples = test_samples[:val_split_idx]\n",
        "test_samples = test_samples[val_split_idx:]\n",
        "\n",
        "assert len(words_list) == len(train_samples) + len(validation_samples) + len(test_samples)\n",
        "\n",
        "print(f\"Total training samples : {len(train_samples)}\")\n",
        "print(f\"Total validation samples: {len(validation_samples)}\")\n",
        "print(f\"Total testing samples: {len(test_samples)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5KfhPKpubtH",
        "outputId": "bdfa737a-5e37-483e-fae5-d4f7fdaa9167"
      },
      "outputs": [],
      "source": [
        "sagemaker_path = 'image_text.txt'\n",
        "sage_words_list = []\n",
        "with open(sagemaker_path,'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "split_sage = int(0.9 * len(lines))\n",
        "np.random.shuffle(lines)\n",
        "\n",
        "train_sage_samples = lines[:split_sage]\n",
        "test_sage_samples = lines[split_sage:]\n",
        "\n",
        "val_split_sage = int(0.5 * len(test_sage_samples))\n",
        "validation_sage_samples = test_sage_samples[:val_split_sage]\n",
        "test_sage_samples = test_sage_samples[val_split_sage:]\n",
        "\n",
        "print(f\"Total training samples : {len(train_sage_samples)}\")\n",
        "print(f\"Total validation samples: {len(validation_sage_samples)}\")\n",
        "print(f\"Total testing samples: {len(test_sage_samples)}\")\n",
        "\n",
        "assert len(lines) == len(train_sage_samples) + len(test_sage_samples) + len(validation_sage_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PS89-qkRSy80"
      },
      "outputs": [],
      "source": [
        "base_image_path = os.path.join(base_path,\"words\")\n",
        "def get_image_paths_and_labels(samples):\n",
        "  paths = []\n",
        "  corrected_samples = []\n",
        "  for (i,file_line) in enumerate(samples):\n",
        "    line_split = file_line.strip()\n",
        "    line_split = line_split.split(\" \")\n",
        "\n",
        "    image_name = line_split[0]\n",
        "    partI = image_name.split(\"-\")[0]\n",
        "    partII = image_name.split(\"-\")[1]\n",
        "    img_path = os.path.join(base_image_path,partI,partI+'-'+partII,image_name+\".png\")\n",
        "    if os.path.getsize(img_path):\n",
        "      paths.append(img_path)\n",
        "      corrected_samples.append(file_line.split(\"\\n\")[0])\n",
        "  return paths,corrected_samples\n",
        "\n",
        "train_img_paths, train_labels = get_image_paths_and_labels(train_samples)\n",
        "validation_img_paths, validation_labels = get_image_paths_and_labels(validation_samples)\n",
        "test_img_paths, test_labels = get_image_paths_and_labels(test_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R000vm7Ruo4T"
      },
      "outputs": [],
      "source": [
        "train_sage_img_paths = []\n",
        "train_sage_labels = []\n",
        "test_sage_img_paths = []\n",
        "test_sage_labels = []\n",
        "val_sage_img_paths = []\n",
        "val_sage_labels = []\n",
        "\n",
        "def separate_image_path_and_label_sagemaker(lines,base_sage_image_path):\n",
        "    image_paths = []\n",
        "    image_labels = []\n",
        "    for line in lines:\n",
        "        image_path,image_label = line.split('\\t')\n",
        "        full_image_path = os.path.join(base_sage_image_path,image_path)\n",
        "        image_paths.append(full_image_path)\n",
        "        image_labels.append(image_label.strip())\n",
        "    return image_paths,image_labels\n",
        "\n",
        "base_sage_image_path = \"extracted_images/\"\n",
        "train_sage_img_paths,train_sage_labels = separate_image_path_and_label_sagemaker(train_sage_samples,base_sage_image_path)\n",
        "test_sage_img_paths,test_sage_labels = separate_image_path_and_label_sagemaker(test_sage_samples,base_sage_image_path)\n",
        "val_sage_img_paths,val_sage_labels = separate_image_path_and_label_sagemaker(validation_sage_samples,base_sage_image_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ffusuf2iuuGR"
      },
      "outputs": [],
      "source": [
        "train_img_paths.extend(train_sage_img_paths)\n",
        "test_img_paths.extend(test_sage_img_paths)\n",
        "validation_img_paths.extend(val_sage_img_paths)\n",
        "\n",
        "train_labels.extend(train_sage_labels)\n",
        "test_labels.extend(test_sage_labels)\n",
        "validation_labels.extend(val_sage_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Oz2yApASy81",
        "outputId": "d54b0f74-69e4-44fd-9c87-0d77c948e792"
      },
      "outputs": [],
      "source": [
        "train_labels_cleaned=[]\n",
        "characters = set()\n",
        "max_len = 0\n",
        "\n",
        "for label in train_labels:\n",
        "  label = label.split(\" \")[-1].strip()\n",
        "  for char in label:\n",
        "    characters.add(char)\n",
        "\n",
        "  max_len = max(max_len,len(label))\n",
        "  train_labels_cleaned.append(label)\n",
        "\n",
        "print(\"maximum length: \",max_len)\n",
        "print(\"Vocab size: \",len(characters))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X7Ur8_exSy81"
      },
      "outputs": [],
      "source": [
        "def clean_labels(labels):\n",
        "  cleaned_labels = []\n",
        "  for label in labels:\n",
        "    label = label.split(\" \")[-1].strip()\n",
        "    cleaned_labels.append(label)\n",
        "  return cleaned_labels\n",
        "\n",
        "validation_labels_cleaned = clean_labels(validation_labels)\n",
        "test_labels_cleaned = clean_labels(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwKqS7sRSy81"
      },
      "source": [
        "RUN BELOW CELL IF RUNNING FOR FIRST TIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pjvir37BSy82"
      },
      "outputs": [],
      "source": [
        "# #if running model for first time run this cell to save the list of characters and use this save file for for next runs\n",
        "\n",
        "# ff = list(characters)\n",
        "# filename = 'characters_list.pkl'\n",
        "# with open(filename, 'wb') as file:\n",
        "#     pickle.dump(ff, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nbnk694fSy82",
        "outputId": "0b1af08e-ed52-4826-a4d7-5c2e47b32855"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the list from the file\n",
        "with open('characters_list.pkl', 'rb') as file:\n",
        "    ff_loaded = pickle.load(file)\n",
        "\n",
        "# Verify if the loaded list is the same as the original\n",
        "print(ff_loaded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPXzRXkxSy83"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "#mapping characters to integers\n",
        "char_to_num = StringLookup(vocabulary = ff_loaded,mask_token = None)\n",
        "\n",
        "#mapping integers back to original characters\n",
        "num_to_char = StringLookup(\n",
        "    vocabulary = char_to_num.get_vocabulary(),mask_token=None, invert = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f1i4AxsSSy83"
      },
      "outputs": [],
      "source": [
        "def distortion_free_resize(image,img_size):\n",
        "  w,h = img_size\n",
        "  image = tf.image.resize(image,size=(h,w),preserve_aspect_ratio = True)\n",
        "\n",
        "  #check the amount of padding needed to be done.\n",
        "  pad_height = h - tf.shape(image)[0]\n",
        "  pad_width = w - tf.shape(image)[1]\n",
        "\n",
        "  # only necessary if you want to do same amount of padding on both sides.\n",
        "  if pad_height % 2 != 0:\n",
        "    height = pad_height // 2\n",
        "    pad_height_top = height + 1\n",
        "    pad_height_bottom = height\n",
        "  else:\n",
        "    pad_height_top = pad_height_bottom = pad_height//2\n",
        "  if pad_width %2 != 0:\n",
        "    width = pad_width//2\n",
        "    pad_width_left = width + 1\n",
        "    pad_width_right = width\n",
        "  else:\n",
        "    pad_width_left = pad_width_right = pad_width//2\n",
        "\n",
        "  image = tf.pad(\n",
        "      image,\n",
        "      paddings = [\n",
        "          [pad_height_top, pad_height_bottom],\n",
        "          [pad_width_left, pad_width_right],\n",
        "          [0,0],\n",
        "      ],\n",
        "  )\n",
        "\n",
        "  image = tf.transpose(image, perm= [1,0,2])\n",
        "  image = tf.image.flip_left_right(image)\n",
        "  return image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "S_14T2iBSy83"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "padding_token = 99\n",
        "image_width = 128\n",
        "image_height = 32\n",
        "\n",
        "def preprocess_image(image_path, img_size=(image_width,image_height)):\n",
        "  image = tf.io.read_file(image_path)\n",
        "  image = tf.image.decode_png(image,1)\n",
        "  image = distortion_free_resize(image,img_size)\n",
        "  image = tf.cast(image,tf.float32)/255.0\n",
        "  return image\n",
        "\n",
        "def vectorize_label(label):\n",
        "  label = char_to_num(tf.strings.unicode_split(label,input_encoding = \"UTF-8\"))\n",
        "  length = tf.shape(label)[0]\n",
        "  pad_amount = max_len - length\n",
        "  label = tf.pad(label, paddings = [[0,pad_amount]],constant_values=padding_token)\n",
        "  return label\n",
        "\n",
        "def process_images_labels(image_path,label):\n",
        "  image = preprocess_image(image_path)\n",
        "  label = vectorize_label(label)\n",
        "  return{\"image\": image, \"label\" : label}\n",
        "\n",
        "def prepare_dataset(image_paths, labels):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((image_paths,labels)).map(\n",
        "      process_images_labels,num_parallel_calls=AUTOTUNE\n",
        "  )\n",
        "  return dataset.batch(batch_size).cache().prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rINoiLJ2TsF1",
        "outputId": "7c941e4e-3a41-4a2d-9e54-a39a3e6a1f9e"
      },
      "outputs": [],
      "source": [
        "train_img_paths[:4]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "n0VWE99cSy83"
      },
      "outputs": [],
      "source": [
        "train_ds = prepare_dataset(train_img_paths,train_labels_cleaned)\n",
        "validation_ds = prepare_dataset(validation_img_paths,validation_labels_cleaned)\n",
        "test_ds = prepare_dataset(test_img_paths, test_labels_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "9AFsZ3giSy83",
        "outputId": "8f1d95ed-96cb-40d3-d1f3-8e9ed74c514a"
      },
      "outputs": [],
      "source": [
        "for data in train_ds.take(1):\n",
        "  images,labels = data[\"image\"],data[\"label\"]\n",
        "  _,ax=plt.subplots(4,4,figsize=(15,8))\n",
        "  for i in range(16):\n",
        "    img = images[i]\n",
        "    img = tf.image.flip_left_right(img)\n",
        "    img = tf.transpose(img,perm=[1,0,2])\n",
        "    img = (img * 255.0).numpy().clip(0,255).astype(np.uint8)\n",
        "    img = img[:,:,0]\n",
        "\n",
        "    #gather indices where label!=padding_token.\n",
        "    label = labels[i]\n",
        "    indices = tf.gather(label,tf.where(tf.math.not_equal(label,padding_token)))\n",
        "    #convert to string\n",
        "    label = tf.strings.reduce_join(num_to_char(indices))\n",
        "    label = label.numpy().decode(\"utf-8\")\n",
        "\n",
        "    ax[i//4, i %4].imshow(img,cmap=\"gray\")\n",
        "    ax[i//4,i%4].set_title(label)\n",
        "    ax[i//4,i%4].axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZG6NHU6BSy84"
      },
      "outputs": [],
      "source": [
        "class CTCLayer(keras.layers.Layer):\n",
        "  def __init__(self,name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.loss_fn = keras.backend.ctc_batch_cost\n",
        "\n",
        "  def call(self,y_true,y_pred):\n",
        "    batch_len = tf.cast(tf.shape(y_true)[0],dtype=\"int64\")\n",
        "    input_length = tf.cast(tf.shape(y_pred)[1],dtype=\"int64\")\n",
        "    label_length = tf.cast(tf.shape(y_true)[1],dtype=\"int64\")\n",
        "\n",
        "    input_length = input_length * tf.ones(shape=(batch_len,1),dtype=\"int64\")\n",
        "    label_length = label_length * tf.ones(shape=(batch_len,1),dtype=\"int64\")\n",
        "    loss = self.loss_fn(y_true,y_pred,input_length,label_length)\n",
        "    self.add_loss(loss)\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2njgfVIASy84",
        "outputId": "e78cb5d7-e97b-4acd-9b09-9c145ee0a8b0"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "  #inputs to the model\n",
        "  input_img = keras.Input(shape=(image_width,image_height,1),name = \"image\")\n",
        "  labels = keras.layers.Input(name=\"label\",shape = (None,))\n",
        "\n",
        "  #first conv block\n",
        "  x = keras.layers.Conv2D(\n",
        "      32,\n",
        "      (3,3),\n",
        "      activation = \"relu\",\n",
        "      kernel_initializer = \"he_normal\",\n",
        "      padding = \"same\",\n",
        "      name = \"Conv1\",\n",
        "  )(input_img)\n",
        "  x = keras.layers.MaxPooling2D((2,2),name = \"pool1\")(x)\n",
        "\n",
        "  #second conv block\n",
        "  x = keras.layers.Conv2D(\n",
        "      64,\n",
        "      (3,3),\n",
        "      activation = \"relu\",\n",
        "      kernel_initializer = \"he_normal\",\n",
        "      padding = \"same\",\n",
        "      name=\"Conv2\"\n",
        "  )(x)\n",
        "\n",
        "  x = keras.layers.MaxPooling2D((2,2), name = \"pool2\")(x)\n",
        "\n",
        "  new_shape = ((image_width//4),(image_height//4)*64)\n",
        "  x = keras.layers.Reshape(target_shape = new_shape, name = \"reshape\")(x)\n",
        "  x = keras.layers.Dense(64,activation =\"relu\",name = \"dense1\")(x)\n",
        "  x = keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "  # RNNS\n",
        "  x = keras.layers.Bidirectional(\n",
        "      keras.layers.LSTM(128, return_sequences = True, dropout = 0.25)\n",
        "  )(x)\n",
        "  x = keras.layers.Bidirectional(\n",
        "      keras.layers.LSTM(64,return_sequences = True, dropout = 0.25)\n",
        "  )(x)\n",
        "\n",
        "  # +2 to account for the two special tokens introduced by the CTC loss\n",
        "\n",
        "  x = keras.layers.Dense(\n",
        "      len(char_to_num.get_vocabulary()) + 2, activation = \"softmax\", name = \"dense2\"\n",
        "  )(x)\n",
        "\n",
        "  # add CTC layer for calculation ctc loss at each step\n",
        "  output = CTCLayer(name=\"ctc_loss\")(labels,x)\n",
        "\n",
        "  #define the model\n",
        "  model = keras.models.Model(\n",
        "      inputs=[input_img,labels],outputs=output , name = \"handwriting_recognizer\"\n",
        "  )\n",
        "\n",
        "  #optimizer\n",
        "  opt = keras.optimizers.Adam()\n",
        "\n",
        "  #compile the model and return\n",
        "\n",
        "  model.compile(optimizer=opt)\n",
        "  return model\n",
        "\n",
        "\n",
        "# get the model\n",
        "model = build_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QP4wikZDSy84"
      },
      "outputs": [],
      "source": [
        "validation_images = []\n",
        "validation_labels = []\n",
        "\n",
        "for batch in validation_ds:\n",
        "  validation_images.append(batch[\"image\"])\n",
        "  validation_labels.append(batch[\"label\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "C6keoupqSy84"
      },
      "outputs": [],
      "source": [
        "def calculate_edit_distance(labels,predictions):\n",
        "  saprse_labels = tf.cast(tf.sparse.from_dense(labels),dtype=tf.int64)\n",
        "\n",
        "  input_len = np.ones(predictions.shape[0]) * predictions.shape[1]\n",
        "  predictions_decoded = keras.backend.ctc_decode(\n",
        "      predictions,input_length = input_len, greedy=True\n",
        "  )[0][0][:,:max_len]\n",
        "  sparse_predictions = tf.cast(\n",
        "      tf.sparse.from_dense(predictions_decoded),dtype = tf.int64\n",
        "  )\n",
        "\n",
        "  edit_distances = tf.edit_distance(\n",
        "      sparse_predictions, saprse_labels, normalize = False\n",
        "  )\n",
        "  return tf.reduce_mean(edit_distances)\n",
        "\n",
        "class EditDistanceCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, pred_model, train_losses=None, val_losses=None, val_mean_edit_distance=None):\n",
        "        super().__init__()\n",
        "        self.prediction_model = pred_model\n",
        "        self.train_losses = train_losses if train_losses is not None else []\n",
        "        self.val_losses = val_losses if val_losses is not None else []\n",
        "        self.val_mean_edit_distance = val_mean_edit_distance if val_mean_edit_distance is not None else []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        edit_distances = []\n",
        "\n",
        "        for i in range(len(validation_images)):\n",
        "            labels = validation_labels[i]\n",
        "            predictions = self.prediction_model.predict(validation_images[i])\n",
        "            edit_distances.append(calculate_edit_distance(labels, predictions).numpy())\n",
        "\n",
        "        mean_edit_distance = np.mean(edit_distances)\n",
        "        self.val_mean_edit_distance.append(mean_edit_distance)\n",
        "        self.train_losses.append(logs.get(\"loss\"))\n",
        "        self.val_losses.append(logs.get(\"val_loss\"))\n",
        "        print(\n",
        "            f\"Mean edit distance for epoch {epoch + 1}: {mean_edit_distance:.4f}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "C3_IL7BsSy84"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "model = build_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TU8AQmE9Sy84"
      },
      "outputs": [],
      "source": [
        "prediction_model = keras.models.Model(\n",
        "    model.get_layer(name=\"image\").input, model.get_layer(name = \"dense2\").output\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AgdSF7E7Sy84"
      },
      "outputs": [],
      "source": [
        "edit_distance_callback = EditDistanceCallback(prediction_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbZjDj4bSy84",
        "outputId": "c2bc0eb3-ce2d-445f-e809-234ee12b96ea"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=validation_ds,\n",
        "    epochs = epochs,\n",
        "    callbacks =[edit_distance_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "S41E82GwSy85",
        "outputId": "e556a82a-be2f-45b5-edf8-e1be0b8ba783"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(edit_distance_callback.train_losses, label='Training Loss')\n",
        "plt.plot(edit_distance_callback.val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mCeDlZ9Sy85",
        "outputId": "f0e60bf9-8cc0-4232-94d2-6824d49132c6"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "epochs_trained = 0\n",
        "epochs_trained = epochs_trained + epochs\n",
        "model.save('handwriting_recognizer.h5')\n",
        "prediction_model.save('prediction.h5')\n",
        "# Save loss values (train_losses and edit_distance_callback.val_losses)\n",
        "np.savez(\n",
        "    'loss_values.npz', train_losses=edit_distance_callback.train_losses,\n",
        "    val_losses=edit_distance_callback.val_losses,\n",
        "    val_mean_edit_distance = edit_distance_callback.val_mean_edit_distance,\n",
        "    epochs_trained = epochs\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
